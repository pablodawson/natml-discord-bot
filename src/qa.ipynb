{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import GitbookLoader\n",
    "from loader import NatMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.natml.ai\n",
      "https://docs.natml.ai/unity/\n",
      "Fetching text from https://docs.natml.ai/unity/\n",
      "https://docs.natml.ai/unity/\n",
      "Fetching text from https://docs.natml.ai/unity/\n",
      "https://docs.natml.ai/unity/prelims/getting-started\n",
      "Fetching text from https://docs.natml.ai/unity/prelims/getting-started\n",
      "https://docs.natml.ai/unity/prelims/requirements\n",
      "Fetching text from https://docs.natml.ai/unity/prelims/requirements\n",
      "https://docs.natml.ai/unity/workflows/concepts\n",
      "Fetching text from https://docs.natml.ai/unity/workflows/concepts\n",
      "https://docs.natml.ai/unity/workflows/models\n",
      "Fetching text from https://docs.natml.ai/unity/workflows/models\n",
      "https://docs.natml.ai/unity/workflows/vision\n",
      "Fetching text from https://docs.natml.ai/unity/workflows/vision\n",
      "https://docs.natml.ai/unity/authoring/edge\n",
      "Fetching text from https://docs.natml.ai/unity/authoring/edge\n",
      "https://docs.natml.ai/unity/authoring/cloud\n",
      "Fetching text from https://docs.natml.ai/unity/authoring/cloud\n",
      "https://docs.natml.ai/unity/authoring/distribute\n",
      "Fetching text from https://docs.natml.ai/unity/authoring/distribute\n",
      "https://docs.natml.ai/unity/api/imlpredictor\n",
      "Fetching text from https://docs.natml.ai/unity/api/imlpredictor\n",
      "https://docs.natml.ai/unity/api/mlmodel\n",
      "Fetching text from https://docs.natml.ai/unity/api/mlmodel\n",
      "https://docs.natml.ai/unity/api/mledgemodel\n",
      "Fetching text from https://docs.natml.ai/unity/api/mledgemodel\n",
      "https://docs.natml.ai/unity/api/mlcloudmodel\n",
      "Fetching text from https://docs.natml.ai/unity/api/mlcloudmodel\n",
      "https://docs.natml.ai/unity/api/mlfeature\n",
      "Fetching text from https://docs.natml.ai/unity/api/mlfeature\n",
      "https://docs.natml.ai/unity/api/mlfeaturetype\n",
      "Fetching text from https://docs.natml.ai/unity/api/mlfeaturetype\n",
      "https://docs.natml.ai/unity/api/mlpredictorextensions\n",
      "Fetching text from https://docs.natml.ai/unity/api/mlpredictorextensions\n",
      "https://docs.natml.ai/unity/integrations/media-devices\n",
      "Fetching text from https://docs.natml.ai/unity/integrations/media-devices\n",
      "https://docs.natml.ai/unity/integrations/augmented-reality\n",
      "Fetching text from https://docs.natml.ai/unity/integrations/augmented-reality\n",
      "https://docs.natml.ai/unity/integrations/video-recording\n",
      "Fetching text from https://docs.natml.ai/unity/integrations/video-recording\n",
      "https://docs.natml.ai/unity/insiders/changelog\n",
      "Fetching text from https://docs.natml.ai/unity/insiders/changelog\n",
      "https://docs.natml.ai/unity/insiders/oss\n",
      "Fetching text from https://docs.natml.ai/unity/insiders/oss\n"
     ]
    }
   ],
   "source": [
    "loader = NatMLLoader(\"https://docs.natml.ai/unity\", load_all_paths=True)\n",
    "all_pages_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched 22 documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='Creating Edge Predictors\\nML Where your Users Are\\nAs you might have noticed above, \\nMLEdgeModel\\n instances typically won\\'t be used directly. Instead, they will be used through Edge Predictors, which are lightweight classes that can transform input data into the model\\'s expected input features; and can transform the model\\'s output features into easily usable types. Below are the general steps in implementing Edge predictors:\\nDefining the Predictor\\nAll Edge predictors must\\n inherit from the \\nIMLPredictor<TOutput>\\n interface. The predictor has a single generic type argument, \\nTOutput\\n, which is a developer-friendly type that is returned when a prediction is made. For example, the \\nMobileNetv2Predictor\\n predictor class which classifies an image uses a tuple for its output type:\\n// The MobileNetv2 classification predictor returns a tuple\\nclass\\n \\nMobileNetv2Predictor\\n \\n:\\n \\nIMLPredictor\\n<(\\nstring\\n label\\n,\\n \\nfloat\\n confidence\\n)>\\n \\n{\\n \\n..\\n.\\n \\n}\\nDefining Constructors\\nAll Edge predictors must\\n define one or more constructors that accept one or more \\nMLModel\\n instances, along with any other supplemental data needed to make predictions with the model(s). For example:\\n/// <summary>\\n/// Create a custom predictor.\\n/// </summary>\\n/// <param name=\"model\">ML model used to make predictions.</param>\\npublic\\n CustomPredictor \\n(\\nMLModel\\n model\\n)\\n \\n{\\n \\n..\\n.\\n \\n}\\nWithin the constructor, the model should cast the \\nMLModel\\n to an \\nMLEdgeModel\\n and store a readonly reference to the edge model:\\n// Store a reference to the Edge model\\nprivate\\n \\nreadonly\\n \\nMLEdgeModel\\n model\\n;\\nMaking Predictions\\nAll Edge predictors must\\n implement a public \\nPredict\\n method which accepts a \\nparams MLFeature[]\\n and returns a \\nTOutput\\n, for example:\\n/// <summary>\\n/// Make a prediction with the model.\\n/// </summary>\\n/// <param name=\"inputs\">Input feature.</param>\\n/// <returns>Output label with unnormalized confidence value.</returns>\\npublic\\n \\n(\\nstring\\n label\\n,\\n \\nfloat\\n confidence\\n)\\n Predict \\n(\\nparams\\n \\nMLFeature\\n[]\\n inputs\\n);\\nWithin the \\nPredict\\n method, the predictor should do three things:\\nInput Checking\\nThe predictor should check that the client has provided the correct number of input features, and that the features have the model\\'s \\nexpected types\\n.\\nIf these checks fail, an appropriate exception should be thrown. Do this instead of returning an un-initialized output.\\nPrediction\\nTo make predictions, the predictor must create \\nMLEdgeFeature\\n instances from input features. Creating an \\nMLEdgeFeature\\n typically requires a corresponding \\nMLFeatureType\\n which dictates any required pre-processing when creating the edge feature. You will typically use the model\\'s input feature types for this purpose:\\n// Get or create the native feature type which the model expects\\nMLFeatureType\\n inputType \\n=\\n model\\n.\\ninputs\\n[\\n0\\n];\\n// Create an Edge feature from the input feature\\nusing\\n \\nMLEdgeFeature\\n edgeFeature \\n=\\n \\n(\\ninputFeature \\nas\\n \\nIMLEdgeFeature\\n).\\nCreate\\n(\\ninputType\\n);\\nTo check if a feature can be used for Edge predictions, cast it to an \\nIMLEdgeFeature\\n and check that the result of the cast is not \\nnull\\n.\\nOnce you have created all the required Edge features, you can then make predictions with the \\nMLEdgeModel\\n:\\n// Make a prediction with one or more native input features\\nusing\\n \\nvar\\n outputFeatures \\n=\\n model\\n.\\nPredict\\n(\\nedgeFeature\\n);\\nMarshaling\\nOnce you have output Edge features from the model, you can then marshal the feature data into a more developer-friendly type. This is where most of the heavy-lifting happens in a predictor:\\n// Marshal the output feature data into a developer-friendly type\\nvar\\n arrayFeature \\n=\\n \\nnew\\n \\nMLArrayFeature\\n<\\nfloat\\n>\\n(\\noutputFeatures\\n[\\n0\\n]);\\n// Do stuff with this data...\\n..\\n.\\nFinally, return your predictor\\'s output:\\n// Create the prediction result from the output data\\nTOutput\\n result \\n=\\n \\n..\\n.;\\n// Return it\\nreturn\\n result\\n;\\nDisposing the Predictor\\nAll Edge predictors must\\n define a \\nDispose\\n method, because \\nIMLPredictor\\n implements the \\nIDisposable\\n interface. This method should be used to dispose any explicitly-managed resources used by the predictor. If a predictor does not have any explicitly-managed resources to dispose, then the predictor should hide the \\nDispose\\n method using interface hiding:\\n// Hide the `Dispose` method so that clients cannot use it directly\\nvoid\\n IDisposable\\n.\\nDispose \\n()\\n \\n{\\n \\n}\\nThe predictor \\nmust not\\n \\nDispose\\n any models provided to it. This is the responsibility of the client.\\nWorkflows - \\nPrevious\\nUsing Predictors\\nNext\\n - Authoring\\nCreating Cloud Predictors\\nLast modified \\n22d ago', lookup_str='', metadata={'source': 'https://docs.natml.ai/unity/authoring/edge', 'title': 'Creating Edge Predictors'}, lookup_index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"fetched {len(all_pages_data)} documents.\")\n",
    "# show second document\n",
    "all_pages_data[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=\"KEY HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I create a model\"\n",
    "docsearch = Chroma.from_documents(all_pages_data, embeddings)\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' var model = await MLEdgeModel.Create(\"@natsuite/yolox\");'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=\"KEY HERE\"), chain_type=\"stuff\")\n",
    "query = \"What is the line of code to create a model?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "367e1ba549d9fcd9e2db2d9e910cfffb520f012bc1de27b60de746f9b24363d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
